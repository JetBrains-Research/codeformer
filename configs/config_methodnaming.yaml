checkpoint: "None"

seed: 7
progress_bar_refresh_rate: 1

model:
  configuration:

  encoder: transformer
  decoder: transformer

  transformer:
    d_model: 768
    nhead: 8
    dropout: 0.1
    num_layers: 8
    dim_feedforward: 2048
  big_bird:
    d_model: 768
    nhead: 8
    dropout: 0.1
    num_layers: 8
    dim_feedforward: 2048
    attention_type: "block_sparse"
    block_size: 64
    num_random_blocks: 3
  longformer:
    d_model: 512
    nhead: 8
    dropout: 0.1
    num_layers: 8
    dim_feedforward: 2048
    attention_window: 512
  my_transformer:
    d_model: 768
    nhead: 8
    dropout: 0.1
    num_layers: 8
    dim_feedforward: 2048

data:
  type: 'plain_code'
  root: ''
  path: '../../pretrained/plain_code.vocab'
  base_tokenizer: "microsoft/codebert-base"
  train_new_tokenizer: False
  max_tokenizer_vocab: 20000
  max_code_parts: 512
  max_label_parts: 7
  max_subsequence_size: 10
  max_subsequences_number: 256

wandb:
  project: long-context-transformer
  group: null
  offline: false
  key: ""

trainer:
  gpu: 1
  n_epochs: 5
  patience: 3
  clip_norm: 5
  teacher_forcing: 1.0
  val_every_epoch: 1
  save_every_epoch: 1
  log_every_n_steps: 10

optimizer:
  optimizer: "AdamW"
  nesterov: true
  lr: 0.001
  weight_decay: 0.01
  n_epochs: 3
  decay_gamma: 0.95

train:

  dataloader:
    batch_size: 16
    shuffle: True
    num_workers: 4
    prefetch_factor: 2
    pin_memory: True
    persistent_workers: True

val:

  dataloader:
    batch_size: 16
    num_workers: 4
    pin_memory: True
    persistent_workers: True

  generation:
    mrr_k: 1
    acc_k: 1

test:

  dataloader:
    batch_size: 16
    num_workers: 4
    pin_memory: True
    persistent_workers: True

  generation:
    method: "greedy"
    gen_check_interval: 9600
    bandwidth: 5
    max_steps: 1000
    mrr_k: 1
    acc_k: 1
