checkpoint: "None"

seed: 7
progress_bar_refresh_rate: 1

model:
  configuration:

  encoder: BigBird
  decoder: Transformer

  Transformer:
    d_model: 768
    nhead: 8
    dropout: 0.1
    num_layers: 8
    dim_feedforward: 2048
  BigBird:
    d_model: 768
    nhead: 8
    dropout: 0.1
    num_layers: 8
    dim_feedforward: 2048
    attention_type: "block_sparse"
    block_size: 64
    num_random_blocks: 3
  T5:
    d_model: 768
    nhead: 8
    dropout: 0.1
    num_layers: 8
    dim_feedforward: 2048
  Codeformer:
    d_model: 768
    nhead: 8
    dropout: 0.1
    num_layers: 8
    dim_feedforward: 2048
    use_begin_end_tokens: True

data:
  type: "plain_code_ast"
  root: "/home/Mikhail.Arkhipov/data/python_8k_100k"
  path: "../../pretrained/plain_code_accumulate_spaces.vocab"
  checkpoint_tokenizer: "Salesforce/codet5p-220m"
  train_new_tokenizer: False
  max_tokenizer_vocab: 20000
  max_code_parts: 512
  max_label_parts: 7
  max_subsequence_size: 14
  max_subsequences_number: 384
  programming_language: 'python'
  path_to_tree_sitter: '../vendor/tree-sitter-python'

wandb:
  project: codeformer
  group: null
  offline: true
  key: ""

trainer:
  effective_batch_size: 512
  gpu:
  n_epochs: 5
  patience: 3
  clip_norm: 5
  teacher_forcing: 1.0
  val_every_epoch: 1
  save_every_epoch: 1
  log_every_n_steps: 10

optimizer:
  optimizer: "Momentum"
  nesterov: true
  lr: 0.01
  weight_decay: 0.0001
  n_epochs: 5
  decay_gamma: 0.95

train:

  dataloader:
    batch_size: 16
    shuffle: True
    num_workers: 4
    prefetch_factor: 2
    pin_memory: True
    persistent_workers: True

val:

  dataloader:
    batch_size: 16
    num_workers: 4
    pin_memory: True
    persistent_workers: True

  generation:
    mrr_k: 1
    acc_k: 1

test:

  dataloader:
    batch_size: 16
    num_workers: 4
    pin_memory: True
    persistent_workers: True

  generation:
    method: "greedy"
    gen_check_interval: 9600
    bandwidth: 5
    max_steps: 1000
    mrr_k: 1
    acc_k: 1
