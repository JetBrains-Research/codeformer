checkpoint: "None"

seed: 7
progress_bar_refresh_rate: 1

model:
  configuration:

  encoder:
  decoder: Transformer

  Transformer:
    d_model: 768
    nhead: 8
    dropout: 0.1
    num_layers: 8
    dim_feedforward: 2048
  BigBird:
    d_model: 768
    nhead: 8
    dropout: 0.1
    num_layers: 8
    dim_feedforward: 2048
    attention_type: "block_sparse"
    block_size: 64
    num_random_blocks: 3
  T5:
    d_model: 768
    nhead: 8
    dropout: 0.1
    num_layers: 8
    dim_feedforward: 2048
  codeformer:
    d_model: 768
    nhead: 8
    dropout: 0.1
    num_layers: 8
    dim_feedforward: 2048
    use_begin_end_tokens: True

data:
  type:
  root: 
  path: '../../pretrained/plain_code_accumulate_spaces.vocab'
  checkpoint_tokenizer: "Salesforce/codet5p-220m"
  train_new_tokenizer: False
  max_tokenizer_vocab: 20000
  max_code_parts:
  max_label_parts: 7
  max_subsequence_size: 
  max_subsequences_number:
  programming_language: 'python'
  path_to_tree_sitter: '../vendor/tree-sitter-python'

wandb:
  project: codeformer
  group: null
  offline: true
  key: ""

trainer:
  gpu: 1
  n_epochs: 5
  patience: 3
  clip_norm: 5
  teacher_forcing: 1.0
  val_every_epoch: 1
  save_every_epoch: 1
  log_every_n_steps: 10

optimizer:
  optimizer: 
  nesterov: true
  lr:
  weight_decay:
  n_epochs: 5
  decay_gamma: 0.95

train:

  dataloader:
    batch_size:
    shuffle: True
    num_workers: 4
    prefetch_factor: 2
    pin_memory: True
    persistent_workers: True

val:

  dataloader:
    batch_size:
    num_workers: 4
    pin_memory: True
    persistent_workers: True

  generation:
    mrr_k: 1
    acc_k: 1

test:

  dataloader:
    batch_size:
    num_workers: 4
    pin_memory: True
    persistent_workers: True

  generation:
    method: "greedy"
    gen_check_interval: 9600
    bandwidth: 5
    max_steps: 1000
    mrr_k: 1
    acc_k: 1
