checkpoint: "None"

seed: 7
progress_bar_refresh_rate: 1

model:
  configuration:

  encoder: transformer
  decoder: code_modelling_ast_transformer

  code_modelling_transformer:
    d_model: 768
    nhead: 8
    dropout: 0.1
    num_layers: 8
    dim_feedforward: 2048
    use_begin_end_tokens: True

  code_modelling_ast_transformer:
    d_model: 768
    nhead: 8
    dropout: 0.1
    num_layers: 8
    dim_feedforward: 2048
    use_begin_end_tokens: True

data:
  type: 'plain_code_modelling_ast'
  root: '/home/tim/codeformer/datasets/python_starcoder_processed'
  programming_language: 'python'
  path_to_tree_sitter: '../vendor/tree-sitter-python'
  path: '../../pretrained/plain_code_accumulate_spaces.vocab'
  base_tokenizer: "microsoft/codebert-base"
  checkpoint_tokenizer: "Salesforce/codet5p-220m"
  train_new_tokenizer: False
  max_tokenizer_vocab: 20000
  max_code_parts: 1024
  max_label_parts: 7
  max_subsequence_size: 10
  max_subsequences_number: 256

wandb:
  project: long-context-transformer
  group: null
  offline: true
  key: ""

trainer:
  gpu: 1
  n_epochs: 5
  patience: 3
  clip_norm: 5
  teacher_forcing: 1.0
  val_every_epoch: 1
  save_every_epoch: 1
  log_every_n_steps: 10

optimizer:
  optimizer: "Momentum"
  nesterov: true
  lr: 0.01
  weight_decay: 0.0001
  n_epochs: 5
  decay_gamma: 0.95

train:

  dataloader:
    batch_size: 8
    shuffle: True
    num_workers: 4
    prefetch_factor: 2
    pin_memory: True
    persistent_workers: True

val:

  dataloader:
    batch_size: 8
    num_workers: 4
    pin_memory: True
    persistent_workers: True

  generation:
    mrr_k: 1
    acc_k: 1

test:

  dataloader:
    batch_size: 8
    num_workers: 4
    pin_memory: True
    persistent_workers: True

  generation:
    method: "greedy"
    gen_check_interval: 9600
    bandwidth: 5
    max_steps: 1000
    mrr_k: 1
    acc_k: 1
