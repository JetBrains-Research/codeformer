GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/tim/anaconda3/envs/JetNN/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/tim/JetNN/jetnn/runners/wandb/run-20230706_161857-xwkpccs2/files exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [6]
  | Name     | Type                           | Params
------------------------------------------------------------
0 | _encoder | MethodNameMyTransformerEncoder | 51.0 M
1 | _decoder | MethodNameTransformerDecoder   | 85.2 M
2 | _metrics | MetricCollection               | 0
3 | _loss    | SequenceCrossEntropyLoss       | 0
------------------------------------------------------------
136 M     Trainable params
0         Non-trainable params
136 M     Total params
544.435   Total estimated model params size (MB)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19400/19400 [00:00<00:00, 247177.95it/s]
  0%|                                                                                                                                                                                                                                                 | 0/19400 [00:00<?, ?it/s]
/home/tim/anaconda3/envs/JetNN/lib/python3.11/site-packages/torch/nn/modules/transformer.py:296: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)s]
  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)
/home/tim/anaconda3/envs/JetNN/lib/python3.11/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/home/tim/anaconda3/envs/JetNN/lib/python3.11/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
Sanity Checking DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.51it/s]
val
---------------
loss = 10.97
f1 = 0.0
precision = 0.0
recall = 0.0
chrf = 5.93
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 165300/165300 [00:00<00:00, 360267.43it/s]
Epoch 0:   0%|                                                                                                                                                                                             | 2/11545 [00:01<2:29:30,  1.29it/s, loss=10.7, v_num=ccs2, f1=0.000]
/home/tim/anaconda3/envs/JetNN/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:234: UserWarning: You called `self.log('train/precision', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.





































































































































































































































































































































































































































































































































































































































































































































