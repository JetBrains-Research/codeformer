GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/tim/anaconda3/envs/JetNN/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/tim/JetNN/jetnn/runners/wandb/run-20230706_162255-yjxbvye3/files exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [7]
  | Name     | Type                         | Params
----------------------------------------------------------
0 | _encoder | MethodNameBigBirdEncoder     | 61.7 M
1 | _decoder | MethodNameTransformerDecoder | 85.2 M
2 | _metrics | MetricCollection             | 0
3 | _loss    | SequenceCrossEntropyLoss     | 0
----------------------------------------------------------
146 M     Trainable params
0         Non-trainable params
146 M     Total params
587.469   Total estimated model params size (MB)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19400/19400 [00:00<00:00, 304625.75it/s]
  0%|                                                                                                                                                                                                                                                 | 0/19400 [00:00<?, ?it/s]
Sanity Checking DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.26it/s]
val
---------------
loss = 10.98
f1 = 0.0
precision = 0.0
recall = 0.0
/home/tim/anaconda3/envs/JetNN/lib/python3.11/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.                                            | 0/19400 [00:00<?, ?it/s]
  warnings.warn(
/home/tim/anaconda3/envs/JetNN/lib/python3.11/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 165300/165300 [00:00<00:00, 406395.50it/s]
Epoch 0:   0%|                                                                                                                                                                                             | 2/11545 [00:01<2:42:23,  1.18it/s, loss=10.7, v_num=vye3, f1=0.000]
/home/tim/anaconda3/envs/JetNN/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:234: UserWarning: You called `self.log('train/precision', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.














































































































































































































































































































































































































































































































































































































